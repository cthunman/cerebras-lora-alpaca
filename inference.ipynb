{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595c8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b61fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model in globals(): del model\n",
    "\n",
    "BASE_MODEL = 'hf_ckpt-half'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad25827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/lxe/miniconda3/envs/llama-lora/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/lxe/miniconda3/envs/llama-lora/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = 'cerebras/Cerebras-GPT-2.7B'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad262f37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human:give me a 3 day travel plan for hawaii\n",
      "\n",
      "Assistant:A 3 day trip to Hawaii would include a stop at the Koolau Valley on Kauai, an island that is popular among travelers due to its breathtaking beaches and world-renowned marine life. From there, you could visit some of the most beloved islands in the state, such as the Big Island, Molokai, Lanai, and Maui. In addition, you can take a boat tour of the Hawaiian Islands from Kahului Harbor to Kohala Point, where you can take in the view of lush green mountains and coastline. Then, head to the Big island of Hawaii and spend a relaxing day in Waikiki Beach, which is one of the top beaches in the world. On your last day in Hawaii, you can enjoy some of the most famous and beautiful beaches in the stateâ€”such as Honokowai, Makawaoa, Kalaeloa, and Kapalua.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Human:give me a 3 day travel plan for hawaii\\n\\nAssistant:\"\n",
    "\n",
    "# Encode the prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "\n",
    "# Generate text based on the encoded prompt\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        do_sample=True,\n",
    "        top_p=0.75,\n",
    "        top_k=85,\n",
    "        temperature=1.99,\n",
    "        typical_p=1,\n",
    "        repetition_penalty=1.3,\n",
    "        max_length=250,  # The maximum number of tokens to generate\n",
    "        num_beams=5,    # The number of beams to use for beam search\n",
    "#         early_stopping=True,  # Stop generation when the model predicts an end-of-sequence token\n",
    "    )\n",
    "\n",
    "# Decode the generated text and print it\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c7b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
